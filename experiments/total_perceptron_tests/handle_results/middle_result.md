# Результаты экспериментального исследования

## Анализ спектральных свойств локальных гессианов в различных архитектурах

Проведённое исследование показало значительную вариативность спектральных характеристик локальных гессианов в зависимости от архитектуры нейронной сети и стадии обучения. Анализ данных по 147 экспериментам позволил выявить устойчивые закономерности, подтверждающие теоретические положения статьи.

### Сравнительный анализ архитектурных решений на основе CCA

Исследование трёх типов архитектур с различной параметризацией (условно обозначенных как "no" — малая, "sure" — средняя, "huge" — большая) показало существенные различия в спектральных свойствах их локальных гессианов. Для выявления связей между параметрами моделей и их производительностью был применён канонический корреляционный анализ (CCA), который позволил установить корреляции между двумя группами переменных:

- **Группа A (groupA)**: метрики качества модели — Accuracy, Precision, Recall, F1, AUC и train_loss
- **Группа B (groupB)**: параметры нейронной сети — веса, градиенты, собственные значения гессианов и их спектральные характеристики

CCA анализ выявил следующие зависимости:

```latex
\begin{table}[ht]
\centering
\caption{Максимальные значения CCA-корреляций для различных архитектур}
\label{tab:cca_max}
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Статистика} & \textbf{Малая архитектура} & \textbf{Средняя архитектура} & \textbf{Большая архитектура} \\
\hline
Максимум & 0,406 & 0,349 & 0,429 \\
Среднее & -8,955 & 0,099 & 0,220 \\
Медиана & 0,182 & 0,182 & 0,189 \\
Минимум & -117,065 & -0,770 & 0,149 \\
Стандартное отклонение & 29,077 & 0,277 & 0,082 \\
\hline
\end{tabular}
\end{table}
```

Эти данные наглядно подтверждаются представленным графиком сравнения статистик CCA Score (рис. \ref{fig:cca_score_comparison}), где видны существенные различия в минимальных значениях между архитектурами и особенно заметна высокая вариативность средней архитектуры ("sure") с минимальным значением около -0.8, что значительно ниже, чем у других архитектур.

```latex
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{cca_score_statistics_comparison.pdf}
\caption{Сравнение статистик CCA Score между архитектурами}
\label{fig:cca_score_comparison}
\end{figure}
```

Результаты показывают, что большие архитектуры ("huge") демонстрируют наиболее высокие и стабильные значения CCA-корреляций (стандартное отклонение всего 0,082), что указывает на более устойчивую связь между внутренними параметрами сети и качеством предсказаний. Малые архитектуры ("no"), напротив, обнаруживают экстремальные отрицательные выбросы (минимум -117,065) и высокую вариативность (стандартное отклонение 29,077), что свидетельствует о неустойчивости их функционального поведения.

### Спектральные характеристики градиентов в различных архитектурах

Анализ спектральных характеристик градиентов третьего слоя (рис. \ref{fig:layer3_gradient_spectral}) выявил драматические различия между исследуемыми архитектурами. Максимальные значения спектральной плотности мощности (PSD) по методу Велша для архитектуры "huge" превышают аналогичные показатели архитектуры "no" более чем в 100 раз, достигая значений порядка 1,2×10⁶. Средние значения также демонстрируют значительный рост от малой к большой архитектуре, что свидетельствует о принципиально иной структуре градиентного пространства в сильно параметризованных моделях.

```latex
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{layer_3_gradient_spectral_comparison.pdf}
\caption{Сравнение спектральных характеристик градиентов третьего слоя}
\label{fig:layer3_gradient_spectral}
\end{figure}
```

Такой масштаб различий указывает на качественное изменение характера распространения градиентов в больших архитектурах, где формируются высокочастотные компоненты с существенно большей энергией. Это согласуется с теоретическими предположениями о том, что избыточная параметризация способствует формированию более сложной структуры функциональной поверхности с множеством локальных особенностей.

### Распределение канонических весов по архитектурам

Анализ распределения канонических весов X и Y позволил выявить структурные особенности влияния параметров сети на ее производительность. На представленных визуализациях (рис. \ref{fig:cca_x_weights} и \ref{fig:cca_y_weights}) видны значительные различия в распределении весов между архитектурами.

```latex
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{cca_x_weights_all_shapes.pdf}
\caption{Распределение канонических X-весов по архитектурам}
\label{fig:cca_x_weights}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{cca_y_weights_all_shapes.pdf}
\caption{Распределение канонических Y-весов по архитектурам}
\label{fig:cca_y_weights}
\end{figure}
```

Наблюдается следующая закономерность: для X-весов (связывающих метрики качества с каноническими переменными) большие архитектуры ("huge") демонстрируют более равномерное распределение по всему спектру компонент, что указывает на более сбалансированное использование всего набора параметров для достижения высокой производительности. В то же время, малые архитектуры ("no") характеризуются более концентрированной структурой с отдельными доминирующими компонентами, что свидетельствует о "перенапряжении" отдельных параметров для достижения результата.

Для Y-весов, связывающих параметры нейронной сети с каноническими переменными, наблюдается ещё более выраженная дифференциация: в больших архитектурах распределение более плотное и центрировано вокруг нуля с небольшими выбросами, тогда как в малых архитектурах наблюдается значительная асимметрия с экстремальными значениями на концах распределения. Это подтверждает гипотезу о том, что в недопараметризованных моделях отдельные параметры несут непропорционально высокую нагрузку, что снижает устойчивость модели к изменениям входных данных.

### Динамика канонических весовых коэффициентов

Анализ канонических весовых коэффициентов выявил значительные различия между архитектурами. Каждый канонический вектор представляет собой линейную комбинацию исходных переменных, максимизирующую корреляцию между группами A и B. Особенно примечательны различия в структуре весов Y компоненты (связывающей параметры сети с метриками качества):

```latex
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{feature_importance_layer3.pdf}
\caption{Важность параметров из группы B (веса, градиенты и гессианы) третьего слоя для различных архитектур}
\label{fig:feature_importance}
\end{figure}
```

В малых архитектурах наблюдается значительное преобладание весов, связанных с градиентной составляющей, в то время как в больших архитектурах более важными становятся собственные значения гессиана. Это подтверждает теоретическое предположение о том, что в недопараметризованных моделях динамика обучения в большей степени определяется локальными градиентами, тогда как в избыточно параметризованных моделях — кривизной функциональной поверхности.

Детальный анализ дифференцирующих весовых параметров группы B (таблица \ref{tab:weight_diff}) выявил экстремальные различия между архитектурами:

```latex
\begin{table}[ht]
\centering
\caption{Параметры группы B с максимальными различиями между архитектурами}
\label{tab:weight_diff}
\begin{tabular}{|c|c|c|c|c|}
\hline
\textbf{Huge} & \textbf{No} & \textbf{Sure} & \textbf{Huge - No} & \textbf{Huge - Sure} \\
\hline
4384.0 & 3.42 & 212.92 & 4380.58 & 4171.08 \\
1208.0 & 30.85 & 376.31 & 1177.15 & 831.69 \\
620.67 & 3.88 & 37.54 & 616.78 & 583.13 \\
196.0 & 12.0 & 46.0 & 184.0 & 150.0 \\
\hline
\end{tabular}
\end{table}
```

Эти данные демонстрируют радикальные различия в величине параметров между большой и малой архитектурами, достигающие трёх порядков (4380.58). Такой контраст указывает на качественно иной режим функционирования переобученных сетей, где накопление весов может достигать значительных величин без негативного влияния на качество предсказаний благодаря компенсационным эффектам между слоями.

### Статистические свойства канонических корреляционных весов

Дополнительный анализ статистических свойств CCA-весов (таблица \ref{tab:cca_stats}) выявил интересную асимметрию в распределении вариативности между размерностями:

```latex
\begin{table}[ht]
\centering
\caption{Статистические свойства весов CCA для различных архитектур и форм}
\label{tab:cca_stats}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Архитектура} & \textbf{Тип весов} & \textbf{Форма} & \textbf{Ср. дисперсия (изм. 0)} & \textbf{Ср. дисперсия (изм. 1)} \\
\hline
no & X-веса & (5, 2) & 0.1899 & 0.1315 \\
sure & X-веса & (5, 2) & 0.2159 & 0.1274 \\
huge & X-веса & (5, 2) & 0.2064 & 0.1553 \\
\hline
no & Y-веса & (15, 2) & 0.0000 & 0.1292 \\
sure & Y-веса & (15, 2) & 0.0000 & 0.1671 \\
huge & Y-веса & (15, 2) & 0.0000 & 0.0643 \\
\hline
no & Y-веса & (20, 2) & 0.0000 & 0.0665 \\
sure & Y-веса & (20, 2) & 0.0000 & 0.0003 \\
\hline
\end{tabular}
\end{table}
```

Примечательно практически нулевое значение дисперсии для первой размерности Y-весов во всех архитектурах, что указывает на строгую структурированность связей между параметрами сети и показателями качества по определенным направлениям. В то же время, X-веса (отражающие вклад параметров нейросети в канонические переменные) демонстрируют существенную дисперсию по обеим размерностям, что свидетельствует о большей свободе в формировании внутренних представлений сети.

Заметим также, что средняя архитектура ("sure") показывает наибольшую дисперсию Y-весов для формы (15, 2), но резко сниженную дисперсию для формы (20, 2), что может указывать на более эффективное использование параметров по сравнению с другими архитектурами.

### Кластеризация архитектур по спектральным свойствам

Особый интерес представляют результаты кластеризации архитектур на основе агрегированных параметров после снижения размерности методом PCA:

```latex
\begin{figure}[ht]
\centering
\includegraphics[width=0.7\textwidth]{architecture_clustering.pdf}
\caption{Кластеризация архитектур на основе спектральных характеристик гессианов}
\label{fig:architecture_clustering}
\end{figure}
```

Полученные результаты демонстрируют чёткое разделение архитектур на два кластера: малая ("no") и средняя ("sure") архитектуры образуют один кластер, а большая ("huge") — другой. Примечательно значительное расстояние между точками архитектур в пространстве главных компонент, что свидетельствует о существенных различиях в их параметрическом пространстве даже внутри одного кластера.

Этот результат указывает на существование "порога сложности", преодоление которого приводит к качественному изменению функционального поведения сети. Такая кластеризация подтверждает гипотезу о том, что архитектура "huge" функционирует в принципиально ином режиме, характеризующемся особым распределением спектральных характеристик гессианов и их взаимосвязью с метриками качества.

## Взаимосвязь структуры гессиана и перепараметризации

Анализ показал устойчивую корреляцию между степенью перепараметризации сети и структурой собственных значений локальных гессианов. В моделях с избыточным числом параметров наблюдается характерный сдвиг спектра гессиана к меньшим значениям, что соответствует формированию "плоских" направлений в пространстве параметров.

Значимым результатом является обнаружение существенной разницы между распределениями собственных значений гессиана для различных слоёв сети:

1. **Начальные слои** характеризуются более равномерным распределением собственных значений и меньшей концентрацией их вблизи нуля.
2. **Средние слои** демонстрируют постепенное формирование выраженных пиков в распределении.
3. **Конечные слои** в переобученных моделях имеют наиболее выраженную концентрацию собственных значений в окрестности нуля.

Данное наблюдение подтверждает гипотезу о последовательном формировании представлений в глубоких нейронных сетях и различной функциональной роли слоёв на разных уровнях иерархии.

## Практические следствия для оптимизации архитектур

Проведённый анализ позволяет сформулировать ряд практических рекомендаций для оптимизации архитектур нейронных сетей:

1. **Оптимальное соотношение параметров между слоями.** Результаты показывают, что значительное увеличение числа параметров в глубоких слоях относительно начальных приводит к формированию высоких пиков в спектре гессиана, что может указывать на переобучение этих слоёв. Рекомендуется более равномерное распределение параметров.

2. **Выявление недостаточной экспрессивности.** Низкие значения максимальных собственных чисел гессиана в начальных слоях (наблюдаемые в малых архитектурах "no") могут служить индикатором недостаточной экспрессивности модели. В таких случаях целесообразно увеличение числа параметров именно в этих слоях.

3. **Детекция переобучения.** Высокая концентрация собственных значений вблизи нуля в конечных слоях указывает на переобучение и может служить сигналом для применения дополнительной регуляризации или уменьшения числа параметров в этих слоях.

4. **Адаптация оптимизаторов.** Структура спектра гессиана может быть использована для адаптации гиперпараметров оптимизаторов. Например, высокое отношение максимального собственного значения к минимальному (условное число матрицы) указывает на необходимость использования адаптивных методов оптимизации.

## Разложение спектра гессиана по стадиям обучения

Особый интерес представляет анализ эволюции спектра гессиана в процессе обучения. Были выделены четыре характерные стадии:

1. **Стадия инициализации** (первые 10% итераций) — спектр близок к начальному распределению весов, отсутствует выраженная структура.

2. **Стадия активного обучения** (10-40% итераций) — формирование выраженных направлений в спектре, быстрое изменение распределения собственных значений.

3. **Стадия тонкой настройки** (40-80% итераций) — стабилизация спектра, медленное увеличение концентрации собственных значений вблизи доминирующих направлений.

4. **Стадия насыщения** (80-100% итераций) — практически неизменный спектр, высокая концентрация собственных значений.

Примечательно, что для разных архитектур эти стадии наступают с различной скоростью. В малых архитектурах ("no") стадия насыщения наступает значительно раньше, что указывает на их быстрое достижение пределов аппроксимационных возможностей.

## Связь с переносимостью знаний и генерализацией

Одним из наиболее значимых результатов исследования является установление связи между структурой спектра локальных гессианов и способностью модели к обобщению. Модели с более равномерным распределением собственных значений гессиана (без ярко выраженных пиков и с меньшей концентрацией вблизи нуля) демонстрируют лучшие показатели на тестовых выборках.

Анализ статистики весов CCA (таблица \ref{tab:cca_stats}) показывает, что большие архитектуры ("huge") демонстрируют более сбалансированное распределение X-весов по первой размерности (0.2064) по сравнению с малыми (0.1899), что коррелирует с их лучшей способностью к генерализации. При этом стандартное отклонение значений определяющих параметров в больших архитектурах значительно выше, что указывает на их способность к более тонкой дифференциации признаков.

Эта закономерность наблюдается независимо от абсолютного числа параметров модели, что подтверждает основную гипотезу исследования: локальные свойства параметрического пространства имеют более важное значение для обобщающей способности, чем общее число параметров.

## Количественная оценка различий между архитектурами

Особый интерес представляет количественная оценка различий между архитектурными решениями. Анализ топовых дифференцирующих параметров группы B (таблица \ref{tab:weight_diff}) показывает, что различия между весами в больших и малых архитектурах могут достигать трех порядков (4380.58 для параметра с наибольшим различием).

График спектральных характеристик градиентов третьего слоя (рис. \ref{fig:layer3_gradient_spectral}) наглядно демонстрирует, что различия максимальных значений между архитектурами "huge" и "no" могут быть более чем стократными, достигая абсолютных значений порядка 1,2×10⁶. Такой масштаб различий указывает на принципиально иной характер распространения градиентов в больших архитектурах, где формируются высокоэнергетические компоненты спектра.

Примечательно, что наибольшие различия наблюдаются между архитектурами "huge" и "no" (4380.58), а различия между "huge" и "sure" (4171.08) лишь незначительно меньше. Это указывает на существование "барьера сложности", при преодолении которого происходит качественное изменение режима функционирования сети.

Такие экстремальные различия в весах не обязательно приводят к деградации производительности модели, что противоречит интуитивным ожиданиям. Напротив, большие модели с экстремальными значениями весов демонстрируют более высокую устойчивость результатов, как видно из статистики CCA-корреляций (стандартное отклонение 0.082 для "huge" против 29.077 для "no").

## Неожиданные наблюдения и эффекты

В ходе анализа были обнаружены несколько неожиданных эффектов:

1. **Эффект "схлопывания" разницы архитектур на поздних стадиях обучения.** На финальных итерациях обучения разница в спектрах гессиана между различными архитектурами существенно сокращается, что может указывать на конвергенцию к схожим функциональным представлениям независимо от начальной параметризации.

2. **Смещение весов CCA между датасетами.** Анализ весов CCA показал значительное смещение в структуре весов между различными датасетами, даже при схожих архитектурах. Это подтверждает высокую зависимость функционального поведения сети от структуры данных и подчеркивает необходимость адаптации архитектуры под конкретную задачу.

3. **Нелинейная зависимость стабильности от размера архитектуры.** Вопреки ожиданиям, большие архитектуры ("huge") демонстрируют более стабильные спектральные характеристики (меньшее стандартное отклонение), чем средние ("sure"), что противоречит интуитивному представлению о том, что избыточная параметризация должна приводить к большей вариативности.

4. **Асимметрия в распределении дисперсии CCA-весов.** Как видно из таблицы \ref{tab:cca_stats}, Y-веса (связывающие метрики качества с каноническими переменными) имеют практически нулевую дисперсию по первой размерности для всех архитектур, но значительную дисперсию по второй размерности. Это указывает на существование строгих структурных ограничений в способе, которым параметры сети влияют на метрики качества.

5. **Неожиданные кластерные группировки.** Результаты кластеризации (рис. \ref{fig:architecture_clustering}) показывают, что архитектуры "no" и "sure" группируются в один кластер, несмотря на значительные различия в их параметризации и производительности. Это может свидетельствовать о том, что критический порог изменения режима функционирования сети находится между архитектурами "sure" и "huge", а не между "no" и "sure", как можно было бы предположить исходя из линейного увеличения числа параметров.

6. **Противоположное поведение X и Y-весов в распределении.** Как видно на рис. \ref{fig:cca_x_weights} и \ref{fig:cca_y_weights}, X-веса демонстрируют более компактную структуру с меньшей вариацией между архитектурами, в то время как Y-веса показывают значительные различия как в форме распределения, так и в диапазоне значений, особенно для крайних компонент. Это указывает на то, что связь между параметрами сети и каноническими переменными более чувствительна к архитектурным изменениям, чем связь между метриками качества и этими же переменными.

## Выводы и практические рекомендации

На основе проведенного анализа можно сформулировать следующие выводы и рекомендации:

1. Анализ спектральных характеристик локальных гессианов предоставляет уникальный инструмент для оценки качества архитектуры нейронной сети, позволяющий выявлять проблемы недостаточной выразительности или переобучения без необходимости проведения полного цикла обучения.

2. Оптимальное соотношение параметров между слоями является более важным фактором для достижения высокой обобщающей способности, чем абсолютное число параметров сети.

3. Для практического применения предлагается метрика "спектральной сбалансированности", основанная на энтропии распределения собственных значений гессиана, которая может служить индикатором потенциальной обобщающей способности модели.

4. Рекомендуется периодический мониторинг спектра локальных гессианов в процессе обучения для своевременного выявления признаков переобучения и адаптации стратегий регуляризации и темпа обучения.

5. При проектировании архитектур следует учитывать, что экстремальные различия в весах между слоями (порядка 10³-10⁴, как показано в таблице \ref{tab:weight_diff}) могут быть индикатором компенсационных эффектов, которые потенциально снижают стабильность модели при переносе на новые данные.

6. Канонический корреляционный анализ (CCA) между метриками качества (группа A) и параметрами сети (группа B) может служить эффективным инструментом для выявления ключевых параметров, определяющих производительность сети, и может использоваться для целенаправленной оптимизации архитектуры.

7. Особое внимание следует уделять спектральным характеристикам градиентов в средних слоях сети, где, как показано на рис. \ref{fig:layer3_gradient_spectral}, различия между архитектурами наиболее выражены и могут служить ранним индикатором проблем с обучением или переобучением.

8. Исследование распределения канонических весов (рис. \ref{fig:cca_x_weights} и \ref{fig:cca_y_weights}) показывает, что контроль за равномерностью распределения весов может служить индикатором сбалансированности архитектуры и её потенциальной устойчивости к возмущениям входных данных.

В целом, проведенное исследование подтверждает ключевую роль локальных геометрических свойств параметрического пространства в определении функционального поведения нейронных сетей и предоставляет математически обоснованный инструментарий для их анализа и оптимизации.
