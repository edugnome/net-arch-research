\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian,english]{babel}
\usepackage{graphicx}
\usepackage{amsmath, amssymb, amsthm, bm}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{geometry}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{mathtools}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{natbib}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}
\newtheorem{assumption}{Assumption}

\newcommand{\Pa}{\mathrm{Pa}} % родительские узлы
\newcommand{\Ch}{\mathrm{Ch}} % дочерние узлы

\title{Полное строгое определение локального и глобального Гессиана второго порядка в произвольной нейронной
архитектуре}

\author{
  \textbf{Аноним} \\
  Аффилиация\\
  \texttt{anonymous@example.com} \\
}

\begin{document}
\sloppy
\maketitle

\begin{abstract}
  В данной статье представлено исчерпывающее математически строгое определение Гессиана второго порядка для
  нейронных сетей произвольной архитектуры, заданной направленным ациклическим графом. Существующие подходы к
  вычислению кривизны функции потерь нейронных сетей часто ограничиваются аппроксимацией Гаусса-Ньютона,
  учитывающей лишь часть вторых производных. В работе разработан полный формализм, учитывающий все чистые и
  смешанные вторые производные по входам и параметрам, кросс-блоки между разными параметрами, а также
  механизмы разделения параметров между узлами сети. Особое внимание уделено негладким активационным функциям
  через использование Clarke-Гессиана. Для тривиального графа из единственного узла без потомков и предков
  предложенные формулы сводятся к стандартному Гессиану $\nabla^2_\theta\mathcal
  L(\theta)\in\mathbb{R}^{p\times p}$. Предложенный формализм предоставляет теоретический фундамент для
  углубленного анализа геометрических свойств функционала потерь и разработки более эффективных алгоритмов
  оптимизации нейронных сетей произвольной архитектуры.
\end{abstract}

\section{Введение}
Гессиан второго порядка $\nabla^2\mathcal L$ играет фундаментальную роль в анализе кривизны функционала
потерь и в разработке методов оптимизации нейронных сетей \citep{martens2014optimizing,
pascanu2013revisiting}. Методы второго порядка, такие как методы Ньютона, trust-region методы и их
модификации, требуют точной информации о кривизне функции потерь для эффективной оптимизации
\citep{nocedal2006numerical}. Однако в контексте глубоких нейронных сетей вычисление и хранение полного
Гессиана становится вычислительно неприемлемым, что приводит к необходимости использования различных аппроксимаций.

Наиболее распространенный подход — аппроксимация Гаусса-Ньютона, которая учитывает лишь часть всех вторых
производных, игнорируя существенные компоненты кривизны \citep{schraudolph2002fast, martens2010deep}. В
данной работе мы предлагаем \emph{полный} формализм, закрывающий следующие пробелы в существующей литературе:

\begin{itemize}
  \item чистые и смешанные вторые производные по \emph{входам} каждого узла нейронной сети;
  \item чистые вторые производные по \emph{параметрам};
  \item кросс-блоки $\partial^2/\partial\theta_v\,\partial\theta_w$ между параметрами разных узлов;
  \item смешанные вход-параметрические производные;
  \item учёт "разделения" (sharing) одного вектора параметров между несколькими узлами;
  \item обработка негладких активационных функций через методологию Clarke-Гессиана.
\end{itemize}

\emph{Особый случай:} Если архитектура нейронной сети вырождается в единственный узел без потомков и предков,
все предлагаемые определения естественным образом сводятся к стандартному Гессиану $\nabla^2_\theta\mathcal
L(\theta)\in\mathbb{R}^{p\times p}$.

\section{Связанные работы}

Изучение геометрии функционала потерь нейронных сетей имеет долгую историю. Классические работы
\citep{amari1998natural, heskes2000natural} заложили основу для использования геометрической информации в
оптимизации нейронных сетей. Особое значение имеет информационная геометрия и натуральный градиентный спуск,
предложенный Амари \citep{amari1998natural}.

В контексте вычисления Гессиана нейронных сетей, значительными являются работы \citep{martens2010deep,
martens2012training}, где представлены эффективные приближения Гессиана для глубоких нейронных сетей.
Гаусс-Ньютон аппроксимация, которая игнорирует вторые производные функции потерь, часто применяется в
практических алгоритмах из-за вычислительной эффективности и гарантии положительной полуопределенности.

Для негладких функций активации, таких как ReLU, традиционный анализ второго порядка неприменим. В работах
\citep{clarke1990optimization, bolte2020conservative} представлен обобщенный подход к недифференцируемым
функциям через субдифференциальное исчисление и Clarke-градиенты. В нашей работе мы применяем эти концепции
непосредственно к нейронным сетям, предлагая полный формализм для анализа кривизны функции потерь.

Недавние работы \citep{ghorbani2019investigation, sagun2017empirical} исследуют спектральные свойства
Гессиана функций потерь нейронных сетей и их связь с обобщением. Наше исследование дополняет эти работы,
предоставляя точный математический аппарат для вычисления всех компонентов Гессиана в произвольных
архитектурах нейронных сетей.

\section{Методология}

\subsection{Таблица обозначений}

Для удобства восприятия сложных формул и структур, приведём систематизированную таблицу основных обозначений:

\begin{table}[h]
  \centering
  \caption{Основные обозначения, используемые в работе}
  \label{tab:notations}
  \begin{tabular}{p{2cm}p{8cm}p{3cm}}
    \toprule
    \textbf{Символ} & \textbf{Определение} & \textbf{Размерность} \\
    \midrule
    $v, w, u$ & Узлы нейронной сети & — \\
    $G = (V, E)$ & Направленный ациклический граф, представляющий нейронную сеть & — \\
    $\text{Pa}(v)$ & Множество родительских узлов узла $v$ & — \\
    $\text{Ch}(v)$ & Множество дочерних узлов узла $v$ & — \\
    $f_v$ & Вектор выходов узла $v$ & $\mathbb{R}^{d_v}$ \\
    $\theta_v$ & Вектор параметров узла $v$ & $\mathbb{R}^{p_v}$ \\
    $\mathcal{L}$ & Функция потерь & $\mathbb{R}$ \\
    $\delta_v$ & Градиент потерь по выходу узла $v$ & $\mathbb{R}^{d_v}$ \\
    $D_{u \gets v}$ & Якобиан преобразования от узла $v$ к узлу $u$ & $\mathbb{R}^{d_u \times d_v}$ \\
    $D_v$ & Якобиан выхода узла $v$ по его параметрам & $\mathbb{R}^{d_v \times p_v}$ \\
    $T_{u;v}$ & Тензор вторых производных выхода узла $u$ по входу от узла $v$ & $\mathbb{R}^{d_u \times d_v
    \times d_v}$ \\
    $T_{u;v,w}$ & Тензор смешанных вторых производных по разным входам & $\mathbb{R}^{d_u \times d_v \times d_w}$ \\
    $T_{v;w,\theta}$ & Тензор смешанных производных по входу и параметрам & $\mathbb{R}^{d_v \times d_w \times p_v}$ \\
    $T_v^{\theta}$ & Тензор вторых производных по параметрам & $\mathbb{R}^{d_v \times p_v \times p_v}$ \\
    $H^f_{v,w}$ & Блок входного Гессиана между узлами $v$ и $w$ & $\mathbb{R}^{d_v \times d_w}$ \\
    $H_{\theta_v,\theta_w}$ & Блок параметрического Гессиана & $\mathbb{R}^{p_v \times p_w}$ \\
    $\partial_C^2 f_v$ & Clarke-Гессиан узла $v$ (для негладкого случая) & множество матриц \\
    \bottomrule
  \end{tabular}
\end{table}

\begin{remark}[Соглашение об индексах]
  В работе приняты следующие соглашения об индексах:
  \begin{itemize}
    \item $i$ — индекс компоненты выхода узла ($f_v$ или $f_u$)
    \item $j, k$ — индексы компонент входов узлов
    \item $k, \ell$ — в контексте параметров, индексы компонент параметра $\theta_v$
    \item $v, w, u$ — индексы узлов в графе нейронной сети
  \end{itemize}
\end{remark}

\subsection{Функциональные пространства и аналитические предпосылки}

Прежде чем перейти к определению компонентов и структуры Гессиана, необходимо формализовать функциональные
пространства, в которых рассматривается задача, и уточнить аналитические предпосылки анализа.

\begin{definition}[Функциональные пространства]
  В рамках данной работы рассматриваются следующие функциональные пространства:
  \begin{itemize}
    \item $\mathbb{R}^n$ с евклидовой нормой $\|\cdot\|_2$ — конечномерное гильбертово пространство
      параметров, активаций и градиентов.
    \item $C^2(\mathbb{R}^n, \mathbb{R}^m)$ — пространство дважды непрерывно дифференцируемых функций из
      $\mathbb{R}^n$ в $\mathbb{R}^m$, используемое для гладкого случая.
    \item $C^{1,1}(\mathbb{R}^n, \mathbb{R}^m)$ — пространство непрерывно дифференцируемых функций с
      липшицевыми производными, используемое для негладкого случая.
    \item $PC^2(\mathbb{R}^n, \mathbb{R}^m)$ — пространство кусочно дважды дифференцируемых функций, где
      каждый кусок принадлежит $C^2$, а границы кусков образуют множество меры нуль.
    \item $L(\mathbb{R}^n, \mathbb{R}^m)$ — пространство линейных операторов (матриц) из $\mathbb{R}^n$ в
      $\mathbb{R}^m$ с операторной нормой и нормой Фробениуса.
  \end{itemize}
\end{definition}

\begin{assumption}[Регулярность функций узлов]
  \label{ass:regularity}
  Для каждого узла $v \in V$ нейронной сети:
  \begin{enumerate}
    \item В гладком случае (Случай A): функция узла $g_v \in C^2(\mathbb{R}^{\sum_{u\in\Pa(v)} d_u},
      \mathbb{R}^{d_v})$, т.е.
      дважды непрерывно дифференцируема по всем входам и параметрам.
    \item В негладком случае (Случай B): функция узла $g_v \in PC^2(\mathbb{R}^{\sum_{u\in\Pa(v)} d_u},
      \mathbb{R}^{d_v})$ и локально липшицева, т.е. является кусочно дважды
      дифференцируемой с границами кусков, образующими множество меры нуль.
  \end{enumerate}
\end{assumption}

\begin{proposition}[Существование и непустота Clarke-субдифференциала]
  \label{prop:clarke_existence}
  Для локально липшицевой функции $f: \mathbb{R}^n \to \mathbb{R}$, которая дифференцируема почти всюду (в
  смысле меры Лебега по теореме Радемахера), Clarke-субдифференциал $\partial_C f(x)$ определен и непуст во
  всех точках $x \in
  \mathbb{R}^n$. Более того, $\partial_C f(x)$ является выпуклым компактным множеством в метрическом
  пространстве $(L(\mathbb{R}^n, \mathbb{R}), \|\cdot\|_{op})$, где $\|\cdot\|_{op}$ — операторная норма.

  Для векторнозначных функций $F: \mathbb{R}^n \to \mathbb{R}^m$ субдифференциал определяется покомпонентно,
  и Clarke-Гессиан $\partial_C^2 F(x)$ также существует при соответствующих условиях локальной липшицевости и
  почти всюду дифференцируемости компонент градиента $\nabla F_i$.
\end{proposition}

Эти предпосылки гарантируют корректность всех последующих определений и вычислений, связанных с
дифференцированием функций узлов нейронной сети как в гладком, так и в негладком случаях.

\subsection{Модель нейронной сети и обозначения}

\begin{definition}[Архитектура нейронной сети]
  Рассматривается нейронная сеть, архитектура которой представлена в виде направленного ациклического графа
  (DAG) $G = (V, E)$, где $V$ — множество узлов сети, а $E$ — множество направленных рёбер.
\end{definition}

Для каждого узла $v \in V$ определены следующие компоненты:
\begin{description}
  \item[Входы:] $f_{\Pa(v)}\in\prod_{u\in\Pa(v)}\mathbb{R}^{d_u}$, где $\Pa(v)$ — множество родительских узлов для $v$.
  \item[Параметры:] $\theta_v\in\mathbb{R}^{p_v}$ — параметры, связанные с узлом $v$.
  \item[Функция узла:] $f_v = g_v\bigl(f_{\Pa(v)},\,\theta_v\bigr)\in\mathbb{R}^{d_v}$ — отображение,
    преобразующее входы и параметры в выход узла $v$.
  \item[Функция потерь:] $\mathcal L:\mathbb{R}^{d_{out}}\to\mathbb{R}$ — функция потерь, определённая на
    выходном узле $out \in V$.
\end{description}

В зависимости от гладкости функций узлов, выделяем два принципиально различных случая:

\begin{itemize}
  \item \textbf{Случай A (гладкий).} Все функции узлов $g_v$ дважды непрерывно дифференцируемы по входам и
    параметрам, т.е. $g_v \in C^2$.
  \item \textbf{Случай B (негладкий).} В сети присутствуют негладкие функции активации, такие как ReLU,
    max-pooling и другие. В этом случае используется концепция Clarke-Гессиана $\partial_C^2 f_v$.
\end{itemize}

Вычисление всех блоков Гессиана осуществляется в \emph{обратном топологическом порядке} по графу $G$, начиная
с выходного узла $out$.

\subsection{Градиенты первого порядка}

Для разработки полного формализма Гессиана второго порядка необходимо сначала определить производные первого
порядка, которые служат основой для дальнейших вычислений:

\begin{align*}
  &\delta_v \;:=\; \nabla_{f_v}\,\mathcal L
  &&\in\mathbb{R}^{d_v}, \quad \text{(градиент потерь по выходу узла $v$)}
  \\[-2pt]
  &\delta_{v,i} \;:=\; [\delta_v]_i,
  &&i=1,\dots,d_v, \quad \text{(компоненты градиента)}
  \\[3pt]
  &D_{u\gets v}
  \;:=\;\frac{\partial f_u}{\partial f_v}
  &&\in\mathbb{R}^{d_u\times d_v}, \quad \text{(якобиан по входу)}
  \\[-2pt]
  &D_v
  \;:=\;\frac{\partial f_v}{\partial\theta_v}
  &&\in\mathbb{R}^{d_v\times p_v}. \quad \text{(якобиан по параметрам)}
\end{align*}

Градиенты $\delta_v$ и якобианы $D_{u\gets v}$, $D_v$ являются основой цепного правила первого порядка и
используются для вычисления производных функции потерь по параметрам сети.

\subsection{Тензоры вторых производных}

Для полного учёта всех вторых производных функций узлов вводятся следующие тензорные структуры:

\begin{align*}
  &[T_{u;v}]_{i,j,k}
  = \frac{\partial^2 (f_u)_i}{\partial(f_v)_j\,\partial(f_v)_k}
  \quad\in\mathbb{R}^{d_u\times d_v\times d_v},
  &&v\in\Pa(u),
  \\[3pt]
  &[T_{u;\,v,w}]_{i,j,k}
  = \frac{\partial^2 (f_u)_i}{\partial(f_v)_j\,\partial(f_w)_k}
  \quad\in\mathbb{R}^{d_u\times d_v\times d_w},
  &&v,w\in\Pa(u),\ v\neq w,
  \\[3pt]
  &[T_{v;\,w,\theta}]_{i,j,k}
  = \frac{\partial^2 (f_v)_i}{\partial(f_w)_j\,\partial(\theta_v)_k}
  \quad\in\mathbb{R}^{d_v\times d_w\times p_v},
  &&w\in\Pa(v),
  \\[3pt]
  &[T_v^\theta]_{i,k,\ell}
  = \frac{\partial^2 (f_v)_i}{\partial(\theta_v)_k\,\partial(\theta_v)_\ell}
  \quad\in\mathbb{R}^{d_v\times p_v\times p_v}.
\end{align*}

\begin{remark}[Тензорная нотация и правила свертки]
  В тензорных выражениях выше и далее приняты следующие соглашения:
  \begin{itemize}
    \item Индекс $i$ всегда относится к компоненте выхода соответствующего узла ($f_u$ или $f_v$).
    \item Индексы $j$ и $m$ относятся к компонентам входов от родительских узлов.
    \item Индексы $\alpha$ и $\beta$ относятся к компонентам параметров $\theta_v$.
    \item Обозначение $[T]_{i,\bullet,\bullet}$ представляет матрицу (срез тензора), полученную фиксацией индекса $i$.
    \item При умножении на скаляр $\delta_{v,i}$ подразумевается свёртка по индексу $i$ с весовыми
      коэффициентами $\delta_{v,i}$.
  \end{itemize}

  При свертке тензоров с другими тензорами или векторами используются следующие правила:
  \begin{itemize}
    \item Для выражения $[T_{u;v}]_{i,j,k}\delta_{u,i}$ результатом является матрица размерности $d_v \times
      d_v$ с элементами $\sum_{i=1}^{d_u}[T_{u;v}]_{i,j,k}\delta_{u,i}$.
    \item При матричном умножении $D_{u\gets v}^\top H^f_{u,u} D_{u\gets w}$ индексы сворачиваются согласно
      правилам матричного произведения, где $D_{u\gets v}^\top \in \mathbb{R}^{d_v \times d_u}$, $H^f_{u,u}
      \in \mathbb{R}^{d_u \times d_u}$, $D_{u\gets w} \in \mathbb{R}^{d_u \times d_w}$.
    \item Тензорное выражение $\sum_{i=1}^{d_u}[T_{u;v,w}]_{i,\bullet,\bullet}\delta_{u,i}$ преобразуется в
      матрицу размерности $d_v \times d_w$ с элементами $\sum_{i=1}^{d_u}[T_{u;v,w}]_{i,j,k}\delta_{u,i}$.
  \end{itemize}

  Это соглашение обеспечивает однозначность всех тензорных операций в формулах и устраняет возможные
  неоднозначности при переходе от тензорной к матричной записи.
\end{remark}

Эти тензоры учитывают чистые и смешанные вторые производные функций узлов по входам и параметрам. При
суммировании по индексу $i$ с весом $\delta_{u,i}$, эти тензоры дают вклад в Гессиан функции потерь.

\subsection{Clarke-Гессиан для негладких функций активации}

Для негладких функций активации, таких как ReLU, Leaky ReLU или max-pooling, классическое понятие Гессиана
неприменимо в точках негладкости. В этом случае используется концепция Clarke-субдифференциала
\citep{clarke1990optimization}.

\begin{definition}[Обобщенный якобиан и Clarke-Гессиан]
  Для локально липшицевой функции $f: \mathbb{R}^n \to \mathbb{R}^m$, обобщенный якобиан по Кларку в точке
  $x$ определяется как
  \[
    \partial_C f(x) = \mathrm{co}\{\lim_{i\to\infty} \nabla f(x_i) : x_i \to x, x_i \in \mathcal{D}_f\},
  \]
  где $\mathrm{co}$ — выпуклая оболочка, а $\mathcal{D}_f$ — множество точек, где $f$ дифференцируема.

  Clarke-Гессиан для функции $f$ определяется как обобщенный якобиан градиента $\nabla f$ (если он существует):
  \[
    \partial_C^2 f(x) = \partial_C(\nabla f)(x).
  \]
\end{definition}

\begin{theorem}[Существование Clarke-Гессиана для ReLU-сетей]
  Пусть нейронная сеть использует активации $\operatorname{ReLU}(t)=\max\{0,t\}$
  и имеет DAG вычислений $G=(V,E)$.
  Обозначим через $\mathcal L:\mathbb R^{d}\to\mathbb R$ функцию потерь,
  полученную как композицию сети $F:\mathbb R^{d}\to\mathbb R^{m}$ с вне­шней
  функцией $\ell:\mathbb R^{m}\to\mathbb R$:
  \[
    \mathcal L(x)=\ell\bigl(F(x)\bigr).
  \]
  Предположим, что $\ell \in C^2(\mathbb{R}^m, \mathbb{R})$ и локально липшицева. Тогда

  \begin{enumerate}
    \item Каждая функция узла $f_v$ локально липшицева.
    \item Для входов $x$, где отображение $x \mapsto [f_{u_1}(x),\dots,f_{u_k}(x)]^{\!\top}$ имеет локально
      полный ранг для всех узлов, функция $\mathcal L$ дважды дифференцируема в $x$. Без этого рангового
      условия множество точек негладкости может иметь положительную меру, как показывает пример
      1-D ReLU сети $F(x)=\max\{0,x\}$ с $\mathcal L(x)=F(x)^2/2$, где функция дважды недифференцируема
      на $(-\infty,0]$.
    \item Во всех точках $x$, где $\mathcal L$ дважды дифференцируема,
      Clarke-Гессиан $\partial^2_C\mathcal L(x)$ вырождается в одиночное множество,
      совпадающее с обычным Гессианом $\nabla^{2}\mathcal L(x)$.
    \item На подмногообразии нулевой меры, соответствующем границам линейных регионов ReLU,
      Clarke-Гессиан $\partial^2_C\mathcal L(x)$ представляет собой
      непустое, выпуклое и компактное множество матриц при условии, что градиент $\nabla \mathcal L(x)$ существует.
  \end{enumerate}
\end{theorem}

\begin{proof}
  \textbf{Шаг 1 (локальная липшицевость каждого узла).}
  Рассмотрим топологический порядок вершин $v_1,\dots,v_{|V|}$.
  Для входного узла $v_1$ функция~$f_{v_1}(x)=x$ очевидно 1-липшицева.
  Пусть узел $v$ получает выходы $f_{u_1},\dots,f_{u_k}$ предыдущих вершин
  и применяет линейное преобразование $W_v(\,\cdot\,)+b_v$, за которым следует ReLU:
  \[
    f_v(x)=\operatorname{ReLU}\!\bigl(W_v\,[f_{u_1}(x),\dots,f_{u_k}(x)]^{\!\top}+b_v\bigr).
  \]
  Линейное отображение имеет константу Липшица $\|W_v\|_2$, а ReLU — константу 1.
  Следовательно, $f_v$ $\bigl(\prod_{i=1}^k L_{u_i}\bigr)\|W_v\|_2$-липшицева,
  где $L_{u_i}$ — константа для $f_{u_i}$.
  Индукцией по порядку вершин получаем локальную липшицевость всех $f_v$.

  \medskip
  \textbf{Шаг 2 (мера множества гладкости).}
  Заметим, что функция $\mathcal L$ негладка только на подмногообразиях, соответствующих границам линейных
  регионов ReLU, которые в общем случае могут иметь положительную меру. Для каждого нейрона с
  ReLU-активацией множество точек, где предактивация равна нулю, есть решение уравнения вида
  $W_v\,[f_{u_1}(x),\dots,f_{u_k}(x)]^{\!\top}+b_v = 0$. При фиксированных параметрах $W_v$ и $b_v$ и при
  условии, что отображение $x \mapsto [f_{u_1}(x),\dots,f_{u_k}(x)]^{\!\top}$ имеет локально полный ранг,
  это уравнение задаёт гиперповерхность (подмногообразие коразмерности 1) в пространстве входов, имеющую
  нулевую меру Лебега.

  Однако для узких или свёрточных слоёв это ранговое условие может нарушаться. Рассмотрим простой пример:
  1-D ReLU сеть $F(x)=\max\{0,x\}$ с функцией потерь $\mathcal L(x)=F(x)^2/2$. Здесь функция $\mathcal L$
  дважды недифференцируема на $(-\infty,0]$, который имеет положительную меру Лебега. Таким образом,
  утверждение о дифференцируемости "почти всюду" справедливо только при дополнительном ранговом предположении.

  Если ранговое условие выполнено, то согласно результатам \citet{hanin2019complexity} и
  \citet{serra2018bounding}, множество точек
  негладкости ReLU-сети с $L$ слоями и общим числом нейронов $N$ может быть покрыто не более чем $2^N$
  аффинными подпространствами коразмерности 1, каждое из которых имеет меру Лебега нуль.

  Более строго, согласно результатам \citet{hanin2019complexity} и \citet{serra2018bounding}, множество точек
  негладкости ReLU-сети с $L$ слоями и общим числом нейронов $N$ может быть покрыто не более чем $2^N$
  аффинными подпространствами коразмерности 1, каждое из которых имеет меру Лебега нуль. Следовательно, в
  почти всех точках $x$ функция $\mathcal L$ дважды дифференцируема.

  \medskip
  \textbf{Шаг 3 (совпадение Гессианов в гладких точках).}
  Пусть $x$ — точка, где $\mathcal L$ дважды дифференцируема.
  Тогда градиент $\nabla\mathcal L$ непрерывен в окрестности $x$ и дифференцируем в $x$,
  так что по определению обобщённого Гессиана
  \[
    \partial_C^{2}\mathcal L(x)=\bigl\{\nabla^{2}\mathcal L(x)\bigr\}.
  \]
  В общем случае внутри одного линейного региона сети функция $F$ аффинна, т.е. $F(x) = Ax + b$ для некоторых
  $A$ и $b$. Если внешняя функция $\ell \in C^2$, то применяя цепное правило, получаем:
  \[
    \nabla^2 \mathcal L(x) = A^\top \nabla^2 \ell(F(x)) A.
  \]
  Для типичных функций потерь, таких как квадратичная или кросс-энтропийная, $\nabla^2 \ell$ хорошо
  определено и ненулевое.

  \medskip
  \textbf{Шаг 4 (существование Clarke-Гессиана в негладких точках).}
  В отличие от стандартного цепного правила для первых производных \citep{bjarnason2005efficient}, для вторых
  производных композиции функций в негладком случае следует использовать обобщённое цепное правило для
  Clarke-Гессиана \citep{mordukhovich2012generalized, jeyakumar1999generalized}.

  Для функции $\mathcal L(x) = \ell(F(x))$, где $\ell \in C^2$ и $F$ локально липшицева с существующим
  градиентом, Clarke-Гессиан $\partial_C^2\mathcal L(x)$ является непустым, выпуклым и \textbf{компактным}, поскольку
  все множества $\partial_C^2F_i(x)$ ограничены (см. \cite[Thm 3.46]{mordukhovich2012generalized})
  и итоговая выпуклая оболочка конечного объединения ограниченных замкнутых множеств остаётся компактной.

  \medskip
  Таким образом, все четыре утверждения теоремы доказаны.
\end{proof}

\begin{definition}[Clarke-Гессиан с минимальной нормой]
  В негладком случае (Случай B), вместо единственного блока $H^f_{v,v}$ и соответствующих
  $H_{\theta_v,\theta_w}$ получаем множество $\partial_C^2f_v$. Конкретный элемент этого множества выбирается
  из условия минимизации квадрата нормы Фробениуса:
  \[
    H^f_{v,w}
    = \arg\min_{M\in\partial_C^2 f_v}\|M\|_F^2,
    \quad
    H_{\theta_v,\theta_w}
    = \arg\min_{M\in\partial^2_{\theta_v,\theta_w}\!\mathcal L}\|M\|_F^2.
  \]
\end{definition}

\begin{remark}[О единственности элемента минимальной нормы]
  Квадрат нормы Фробениуса $\|M\|_F^2$ является строго выпуклой функцией от $M$, а множество $\partial_C^2
  f_v$ выпукло и компактно. Следовательно, задача минимизации $\|M\|_F^2$ имеет единственное решение, что
  обеспечивает однозначность выбора элемента из субдифференциала.
\end{remark}

\subsection{Полный входной Гессиан}

\begin{definition}[Входной Гессиан]
  Полный входной Гессиан представляет собой блочную матрицу $\{H^f_{v,w}\}_{v,w\in V}$, где каждый блок
  $H^f_{v,w}\in\mathbb{R}^{d_v\times d_w}$ определяется рекурсивно:
\end{definition}

\begin{equation}\label{eq:Hf}
  \boxed{
    \begin{split}
      H^f_{v,w}
      &=
      \sum_{u\in\Ch(v)\cap\Ch(w)}
      D_{u\gets v}^\top\,H^f_{u,u}\,D_{u\gets w}
      \quad\text{(Гаусс-Ньютон)}\\
      &\quad+
      \sum_{u\in\Ch(v)\cap\Ch(w)}
      \sum_{i=1}^{d_u}
      [T_{u;\,v,w}]_{i,\bullet,\bullet}\,\delta_{u,i}
      \quad\text{(смешанные входы)}\\
      &\quad+
      \mathbf{1}_{v=w}\,
      \sum_{u\in\Ch(v)}
      \sum_{i=1}^{d_u}
      [T_{u;v}]_{i,\bullet,\bullet}\,\delta_{u,i}
      \quad\text{(чистые по одному входу)}\\
      &\quad+
      \frac{\partial^2 \mathcal{L}}{\partial f_v \partial f_w}
      \quad\text{(прямая зависимость потерь от узлов)}
    \end{split}
  }
\end{equation}

с базовыми условиями:
\begin{align}
  H^f_{out,out} &= \nabla^2\mathcal L(f_{out}), \nonumber\\
  H^f_{out,v} &= H^f_{v,out} = 0\quad (\forall v\neq out), \nonumber\\
\end{align}

Последнее слагаемое в формуле \eqref{eq:Hf} учитывает случай, когда функция потерь напрямую зависит от
выходов узлов $v$ и $w$, даже если они не имеют общих потомков или один из них является листом.

\begin{theorem}[О ненулевых блоках входного Гессиана]
  Блок $H^f_{v,w}$ может быть ненулевым в любом из следующих случаев:
  \begin{enumerate}
    \item Существует путь от $v$ к некоторому узлу $u$ и путь от $w$ к тому же узлу $u$, формально: $\exists u \in V: v
      \rightarrow^* u \text{ и } w \rightarrow^* u$, где $\rightarrow^*$ обозначает наличие пути в графе $G$.
    \item Существует путь от $v$ к $w$ или от $w$ к $v$, т.е. $v \rightarrow^* w$ или $w \rightarrow^* v$.
    \item Существует функциональная зависимость $\mathcal{L}$ от обоих узлов $f_v$ и $f_w$ напрямую, т.е.
      $\frac{\partial^2 \mathcal{L}}{\partial f_v \partial f_w} \neq 0$.
  \end{enumerate}
\end{theorem}

\begin{proof}
  Рассмотрим формулу \eqref{eq:Hf} для блока $H^f_{v,w}$:

  В случае 1, если существуют пути $v \rightarrow^* u$ и $w \rightarrow^* u$, то будет существовать общий потомок
  $c \in \Ch(v) \cap \Ch(w)$, что даст ненулевой вклад через первые два слагаемых формулы.

  В случае 2, если $v \rightarrow^* w$, то для некоторого узла $u$ на пути имеем $u \in \Ch(v)$ и $w \in \Ch(u)$,
  что дает вклад через третье слагаемое когда $v=u$. Аналогично для $w \rightarrow^* v$.

  В случае 3, последнее слагаемое $\frac{\partial^2 \mathcal{L}}{\partial f_v \partial f_w}$ явно ненулевое.

  Если ни одно из этих условий не выполняется, то изменения выходов $f_v$ и $f_w$ влияют на непересекающиеся
  подмножества переменных, от которых зависит $\mathcal{L}$, и следовательно $H^f_{v,w} = 0$.
\end{proof}

\textbf{Свойство симметрии:} В гладком случае (Случай A) $H^f_{v,w}=(H^f_{w,v})^\top$ для всех $v,w \in V$,
что следует из равенства смешанных частных производных для дважды непрерывно дифференцируемых функций.

В негладком случае (Случай B) для элементов Clarke-Гессиана минимальной нормы симметрия может не выполняться.
В этом случае можно произвести симметризацию: $\hat{H}^f_{v,w} = \frac{1}{2}(H^f_{v,w} + (H^f_{w,v})^\top)$.

\begin{remark}[Об использовании симметризации в негладком случае]
  Следует отметить, что симметризация Clarke-Гессиана изменяет его спектральные свойства. Если исходные
  матрицы $H^f_{v,w}$ и $(H^f_{w,v})^\top$ имеют разные собственные значения, то симметризованная версия
  $\hat{H}^f_{v,w}$ будет иметь другой спектр. Это может влиять на методы оптимизации, использующие обратный
  Гессиан $H^{-1}$, такие как метод Ньютона.

  Симметризация рекомендуется в следующих случаях:
  \begin{itemize}
    \item Когда важно сохранить положительную определенность (если исходные матрицы положительно определены).
    \item При использовании методов, требующих симметричные матрицы (например, алгоритмы на основе разложения
      Холецкого).
  \end{itemize}

  Симметризацию не рекомендуется применять, когда асимметрия Гессиана несет важную информацию о кривизне
  функции потерь в негладких точках или когда требуется точное вычисление направления Ньютона.
\end{remark}

\subsection{Полный параметрический Гессиан}

\begin{definition}[Параметрический Гессиан]
  Полный Гессиан по параметрам $\nabla^2_{\theta}\mathcal L$ разбивается на блоки
  $\{H_{\theta_v,\theta_w}\}$, $H_{\theta_v,\theta_w}\in\mathbb{R}^{p_v\times p_w}$, каждый из которых определяется как:
\end{definition}

\begin{equation}\label{eq:Htheta}
  \boxed{
    \begin{aligned}
      H_{\theta_v,\theta_w}
      &= D_v^{\!\top}\,H^{f}_{v,w}\,D_w
      &&\text{(блок Гаусса–Ньютона)}\\
      &\quad+\mathbf{1}_{v=w}\sum_{i=1}^{d_v}
      \delta_{v,i}\,[T_v^{\theta}]_{i,\bullet,\bullet}
      &&\text{(чистые по }\theta_v)\\
      &\quad+\!\!\!\!
      \sum_{u\in \Pa(v)\cap\Ch(w)}
      \sum_{i=1}^{d_v}\!
      \delta_{v,i}\;
      [T_{v;u,\theta}]_{i,:,:}\,
      (D_{w\gets u}\,D_w)^{\top}
      &&\text{(вход–парам.\ $v\to w$)}\\
      &\quad+\!\!\!\!
      \sum_{u\in \Pa(w)\cap\Ch(v)}
      \sum_{i=1}^{d_w}\!
      \delta_{w,i}\;
      [T_{w;u,\theta}]_{i,:,:}\,
      (D_{v\gets u}\,D_v)^{\top}
      &&\text{(вход–парам.\ $w\to v$)}
  \end{aligned}}
\end{equation}

\begin{remark}[О согласованности размерностей в формуле \eqref{eq:Htheta}]
  При вычислении третьего слагаемого в формуле \eqref{eq:Htheta}, тензор $[T_{v;u,\theta}]_{i,:,:}$ имеет
  размерность $d_u\times p_v$. Для согласованности размерностей необходимо использовать
  $(D_{w\gets u}\,D_w)^{\top}$ размерности $p_w\times d_u$, а не $D_{w\gets u}\,D_w$ (размерности $d_u\times p_w$).
  Это обеспечивает получение матрицы $p_v\times p_w$, соответствующей требуемой размерности блока
  $H_{\theta_v,\theta_w}$.
  Аналогичное замечание справедливо для четвертого слагаемого.
\end{remark}

\begin{theorem}[Сборка локальных блоков в глобальный Гессиан]
  Пусть параметры всей сети
  \[
    \theta =
    \begin{pmatrix}\theta_{v_1}\\ \theta_{v_2}\\ \vdots\\ \theta_{v_n}
    \end{pmatrix}
    \in \mathbb R^P,
    \quad
    P = \sum_{k=1}^n p_{v_k},
  \]
  и функция потерь $\mathcal L=\mathcal L(\theta)\in C^2(\mathbb R^P)$. Обозначим
  \[
    H_{\theta_{v_i},\theta_{v_j}}
    \;=\;
    \frac{\partial^2 \mathcal L}{\partial\theta_{v_i}\,\partial\theta_{v_j}}
    \;\in\;\mathbb R^{p_{v_i}\times p_{v_j}},
    \qquad
    i,j=1,\dots,n.
  \]
  Тогда полный Гессиан $\nabla^2_\theta \mathcal L\in\mathbb R^{P\times P}$
  разбивается на блоки
  \[
    \nabla^2_\theta \mathcal L
    =
    \begin{pmatrix}
      H_{\theta_{v_1},\theta_{v_1}} & H_{\theta_{v_1},\theta_{v_2}} & \cdots & H_{\theta_{v_1},\theta_{v_n}}\\
      H_{\theta_{v_2},\theta_{v_1}} & H_{\theta_{v_2},\theta_{v_2}} & \cdots & H_{\theta_{v_2},\theta_{v_n}}\\
      \vdots & \vdots & \ddots & \vdots\\
      H_{\theta_{v_n},\theta_{v_1}} & H_{\theta_{v_n},\theta_{v_2}} & \cdots & H_{\theta_{v_n},\theta_{v_n}}
    \end{pmatrix}.
  \]
\end{theorem}

\begin{proof}
  По определению Гессиана
  \[
    \nabla^2_\theta \mathcal L
    = \frac{\partial}{\partial\theta}
    \bigl(\nabla_\theta \mathcal L\bigr)
    \quad\in\;\mathbb R^{P\times P},
  \]
  где $\nabla_\theta \mathcal L\in\mathbb R^P$ записывается в виде
  \(\bigl(\partial \mathcal L/\partial\theta_{v_1}, \ldots,
  \partial \mathcal L/\partial\theta_{v_n}\bigr)^\top\).
  Разбиение вектора $\theta$ на блоки по $\theta_{v_i}$ естественным образом
  даёт блочную структуру у матрицы вторых производных:
  \[
    \bigl[\nabla^2_\theta\mathcal L\bigr]_{(v_i),(v_j)}
    = \frac{\partial}{\partial\theta_{v_j}}
    \Bigl(\frac{\partial \mathcal L}{\partial\theta_{v_i}}\Bigr)
    = \frac{\partial^2 \mathcal L}{\partial\theta_{v_i}\,\partial\theta_{v_j}}
    = H_{\theta_{v_i},\theta_{v_j}}.
  \]
  Поскольку $\mathcal L\in C^2$, блоки симметричны:
  \[
    H_{\theta_{v_i},\theta_{v_j}}
    = \Bigl(H_{\theta_{v_j},\theta_{v_i}}\Bigr)^\top.
  \]
  Собирая все $n^2$ блоков, получаем заявленную матрицу.
\end{proof}

\subsection{Разделение параметров между узлами}

В практических архитектурах нейронных сетей часто используется механизм разделения параметров между
различными узлами, например, в сверточных нейронных сетях или при использовании механизма weight tying в
рекуррентных сетях \citep{pascanu2013difficulty}.

\begin{proposition}[Гессиан разделяемых параметров]
  Если вектор параметров $\theta\in\mathbb{R}^p$ разделяется между узлами $\{v_k\}_{k=1}^K$, то итоговый
  Гессиан для этого вектора вычисляется как сумма:
  \[
    H_{\theta,\theta}
    = \sum_{a=1}^K\sum_{b=1}^K
    H_{\theta_{v_a},\,\theta_{v_b}}.
  \]
\end{proposition}

Это правило учитывает все возможные взаимодействия между параметрами, как внутри одного узла, так и между
различными узлами, использующими один и тот же вектор параметров.

\newpage

\section{Алгоритмы вычисления}

\subsection{Общий алгоритм вычисления полного Гессиана}

\begin{algorithm}
  \caption{Вычисление полного Гессиана для нейронной сети}
  \begin{algorithmic}[1]

    \Require Нейронная сеть с DAG $G=(V,E)$, функции узлов $\{g_v\}$, параметры $\{\theta_v\}$, функция
    потерь $\mathcal{L}$
    \Ensure Полный Гессиан $\nabla^2_\theta \mathcal{L}$

    \State Вычислить прямой проход и получить $f_v$ для всех $v \in V$
    \State Вычислить $\delta_{out} = \nabla_{f_{out}} \mathcal{L}$ и $H^f_{out,out} = \nabla^2 \mathcal{L}(f_{out})$
    \State Инициализировать $H^f_{v,w} = 0$ для всех пар $v,w \in V$, $v \neq out$, $w \neq out$

    \For{$v \in V$ в обратном топологическом порядке}
    \State Вычислить $\delta_v$ по цепному правилу
    \For{$w \in V$ такие, что $\Ch(v) \cap \Ch(w) \neq \emptyset$}
    \State Вычислить $H^f_{v,w}$ по формуле \eqref{eq:Hf}
    \EndFor
    \EndFor

    \For{$v \in V$}
    \For{$w \in V$ такие, что существуют пути $v \to u$ и $w \to u$}
    \State Вычислить $H_{\theta_v, \theta_w}$ по формуле \eqref{eq:Htheta}
    \EndFor
    \EndFor

    \State Учесть разделение параметров между узлами
    \State Собрать блоки в полный Гессиан $\nabla^2_\theta \mathcal{L}$
    \State \Return $\nabla^2_\theta \mathcal{L}$
  \end{algorithmic}
\end{algorithm}

\section{Теоретические результаты}

\subsection{Функционально-аналитические свойства Гессиана}

\begin{theorem}[Функционально-аналитические свойства Гессиана]
  При выполнении Предположения \ref{ass:regularity} о регулярности функций узлов, полный Гессиан
  $\nabla^2_\theta \mathcal{L}$ обладает следующими свойствами:
  \begin{enumerate}
    \item В гладком случае (Случай A) Гессиан является непрерывным оператором на $\mathbb{R}^P$, где $P$ -
      общее число параметров сети.
    \item В негладком случае (Случай B) для почти всех точек параметрического пространства (за исключением
      множества меры нуль) Clarke-Гессиан существует и совпадает с обычным Гессианом.
    \item На подмногообразии сингулярных точек (где активационные функции негладкие) Clarke-Гессиан с
      минимальной нормой обеспечивает наилучшее приближение в смысле нормы Фробениуса.
    \item При использовании предложенных формул \eqref{eq:Hf} и \eqref{eq:Htheta} обеспечивается
      согласованность размерностей всех тензорных операций.
  \end{enumerate}
\end{theorem}

\subsection{Интеграция специализированных архитектурных компонентов}

\begin{theorem}[Интеграция специализированных слоёв]
  Следующие архитектитектурные компоненты могут быть представлены в виде узлов DAG и включены в предложенный формализм:
  \begin{enumerate}
    \item \textbf{Batch Normalization}: представляется как узел с двумя типами параметров (масштабирующие и
      сдвиговые) и дополнительными внутренними переменными (статистики батча).
    \item \textbf{Attention-механизмы}: представляются как набор взаимосвязанных узлов, соответствующих
      вычислению весов внимания (softmax) и взвешенной суммы значений.
    \item \textbf{Слои с остаточными соединениями (ResNet)}: моделируются через параллельные пути в графе с
      последующим объединением.
    \item \textbf{Рекуррентные сети}: отображаются на DAG путём развёртывания (unrolling) во времени, где
      каждый временной шаг представляется отдельным подграфом с разделяемыми параметрами.
  \end{enumerate}
\end{theorem}

\begin{proof}[Схема доказательства]
  Для каждого типа слоёв необходимо определить соответствующие функции узлов $g_v$ и их первые и вторые
  производные. Например, для Batch Normalization:
  \[
    g_v(x, \gamma, \beta) = \gamma \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} + \beta
  \]
  где $\mu_B, \sigma_B^2$ — средние и дисперсии по батчу, $\gamma, \beta$ — параметры масштаба и сдвига.

  Якобианы $D_v$ и тензоры вторых производных $T_v$ вычисляются по стандартным правилам дифференцирования для
  каждого типа узлов, после чего применяются общие формулы \eqref{eq:Hf} и \eqref{eq:Htheta}.
\end{proof}

\subsection{Стохастические узлы и вариационные подходы}

\begin{definition}[Стохастический узел]
  Стохастический узел в нейронной сети — это узел $v \in V$, выход которого является случайной величиной с
  распределением, параметризованным выходами родительских узлов:
  \[
    f_v \sim p(f_v | f_{\Pa(v)}, \theta_v)
  \]
\end{definition}

\begin{theorem}[Гессиан со стохастическими узлами]
  Для нейронных сетей со стохастическими узлами Гессиан функции потерь может быть обобщён следующим образом:
  \begin{enumerate}
    \item При использовании подхода максимального правдоподобия формулы \eqref{eq:Hf} и \eqref{eq:Htheta}
      применяются к ожидаемой функции потерь $\mathbb{E}_{f_v \sim p}[\mathcal{L}]$.
    \item В вариационных автоэнкодерах и подобных моделях Гессиан вычисляется для вариационной нижней границы (ELBO):
      \[
        \mathcal{L}_{ELBO} = \mathbb{E}_{q(z|x)}[\log p(x|z)] - D_{KL}(q(z|x) || p(z))
      \]
    \item Для обучения с подкреплением применяется формализм к функции ожидаемой награды, с учётом
      стохастичности политики.
  \end{enumerate}
\end{theorem}

\begin{proposition}[Переключение к детерминированным узлам]
  При использовании техники репараметризации стохастические узлы могут быть преобразованы в детерминированные
  узлы с внешним источником случайности, что позволяет применить стандартный формализм Гессиана.
\end{proposition}

\section{Анализ вычислительной сложности}

\begin{theorem}[Вычислительная сложность]
  Пусть $|V|=n$ — число узлов в DAG, $P = \sum_{v \in V} p_v$ — общее число параметров, $d = \max_{v \in V}
  d_v$ — максимальная размерность выхода узла, $s = \max_{v \in V} |\Pa(v) \cup \Ch(v)|$ — максимальная
  степень узла. Тогда:

  \begin{enumerate}
    \item Временная сложность вычисления полного Гессиана составляет $O(n s d^3 + n s d^2 P + P^2)$ в общем
      случае с плотными тензорами.
    \item Для сетей с поэлементными функциями активации (например, ReLU, sigmoid), где тензоры $T_{u;v}$ и
      $T_{u;v,w}$ диагональны или разреженные со сложностью $O(d)$, общая временная сложность снижается до
      $O(n s d^2 + n s d P + P^2)$, поскольку операция $D_{u\gets v}^\top\,H^f_{u,u}\,D_{u\gets w}$ всё равно
      требует $O(d^2)$ операций даже при диагональных тензорах.
    \item Пространственная сложность хранения полного Гессиана составляет $O(P^2)$.
    \item Для полносвязного DAG ($s = O(n)$) временная сложность составляет $O(n^2 d^3 + n^2 d^2 P + P^2)$ в
      общем случае и $O(n^2 d^2 + n^2 d P + P^2)$ для диагональных тензоров.
  \end{enumerate}
\end{theorem}

\begin{proof}
  \textbf{1. Вычисление входного Гессиана $H^f_{v,w}$:}

  По формуле \eqref{eq:Hf}, для каждой пары узлов $(v,w)$ необходимо:
  \begin{itemize}
    \item Вычислить якобианы $D_{u\gets v}$ и $D_{u\gets w}$ для всех $u \in \Ch(v) \cap \Ch(w)$, что требует
      $O(|\Ch(v) \cap \Ch(w)| \cdot d^2)$ операций.
    \item Умножить матрицы $D_{u\gets v}^\top\,H^f_{u,u} D_{u\gets w}$ для всех $u \in \Ch(v) \cap \Ch(w)$,
      что требует $O(|\Ch(v) \cap \Ch(w)| \cdot d^3)$ операций.
    \item Вычислить свертки тензоров смешанных производных $[T_{u;\,v,w}]_{i,\bullet,\bullet}\,\delta_{u,i}$,
      что требует $O(|\Ch(v) \cap \Ch(w)| \cdot d^3)$ операций с учетом разреженности тензора.
    \item Для случая $v=w$, вычислить свертки тензоров чистых производных
      $[T_{u;v}]_{i,\bullet,\bullet}\,\delta_{u,i}$, что требует $O(|\Ch(v)| \cdot d^3)$ операций.
  \end{itemize}

  Для прореженного DAG с максимальной степенью узла $s$, число узлов $u \in \Ch(v) \cap \Ch(w)$ не превышает
  $\min(|\Ch(v)|, |\Ch(w)|) \leq s$. Поэтому для всех пар узлов $(v,w)$ общая сложность составляет $O(n^2
  \cdot s \cdot d^3)$.

  С учетом разреженности графа, число пар $(v,w)$ с непустым пересечением $\Ch(v) \cap \Ch(w)$ не превышает
  $O(n \cdot s)$, что дает сложность $O(n s d^3)$.

  \textbf{2. Вычисление параметрического Гессиана $H_{\theta_v,\theta_w}$:}

  По формуле \eqref{eq:Htheta}, для каждой пары узлов $(v,w)$ необходимо:
  \begin{itemize}
    \item Вычислить якобианы $D_v$ и $D_w$, что требует $O(d_v \cdot p_v + d_w \cdot p_w)$ операций.
    \item Умножить матрицы $D_v^\top\,H^f_{v,w}\,D_w$, что требует $O(d_v \cdot d_w \cdot (p_v + p_w))$ операций.
    \item Для диагональных блоков $(v=w)$, вычислить свертки тензоров чистых производных по параметрам, что
      требует $O(d_v \cdot p_v^2)$ операций.
    \item Вычислить смешанные производные, что требует $O(|\Pa(v) \cap \Ch(w)| \cdot d_v \cdot d_u \cdot p_v)$ операций.
  \end{itemize}

  Общая сложность для всех пар $(v,w)$ составляет $O(n^2 \cdot d^2 \cdot P)$. Учитывая разреженность графа,
  число пар с ненулевыми блоками снижается до $O(n \cdot s)$, что дает сложность $O(n s d P)$.
  Если $T_{u;v}$ и $T_{u;v,w}$ диагональны (поэлементные активации), временная сложность понижается до
  $O(ns\,dP + P^{2})$ вместо прежнего $O(ns\,d^{2}P)$.

  \textbf{3. Сборка полного Гессиана:}

  Сборка требует $O(P^2)$ операций для размещения всех блоков в общей матрице размера $P \times P$.

  Суммируя все составляющие, получаем общую временную сложность $O(n s d^3 + n s d^2 P + P^2)$.

  Для полносвязного DAG, где $s = O(n)$, сложность возрастает до $O(n^2 d^3 + n^2 d^2 P + P^2)$.

  Пространственная сложность определяется размером полной матрицы Гессиана $P \times P$, т.е. $O(P^2)$.
\end{proof}

\begin{theorem}[Методы снижения вычислительных затрат]
  Для снижения вычислительной сложности вычисления полного Гессиана можно применять следующие подходы:
  \begin{enumerate}
    \item \textbf{Блочная аппроксимация}: вычисление только диагональных блоков $H_{\theta_v, \theta_v}$
      снижает сложность до $O(n d^3 + P d^2)$.
    \item \textbf{Низкоранговая аппроксимация}: аппроксимация офф-диагональных блоков произведением матриц
      малого ранга снижает сложность до $O(n^2 d^3 + n^2 d^2 r + P r)$, где $r \ll P$ — ранг аппроксимации.
    \item \textbf{Гаусс-Ньютон аппроксимация}: использование только первого члена в формулах \eqref{eq:Hf} и
      \eqref{eq:Htheta} снижает сложность и гарантирует положительную полуопределенность.
    \item \textbf{Кронекеровская факторизация}: представление матричных блоков в виде кронекеровских
      произведений матриц меньшего размера.
  \end{enumerate}
\end{theorem}

\section{Анализ сходимости методов оптимизации}

\begin{theorem}[Локальная сходимость методов Ньютона]
  Пусть $\mathcal{L}(\theta) \in C^2$ — функция потерь, и $\theta^*$ — её локальный минимум, такой что
  $\nabla^2_\theta \mathcal{L}(\theta^*) \succ 0$. Тогда метод Ньютона со степенным шагом:
  \[
    \theta_{t+1} = \theta_t - \alpha_t \cdot [\nabla^2_\theta \mathcal{L}(\theta_t)]^{-1} \nabla_\theta
    \mathcal{L}(\theta_t)
  \]
  имеет квадратичную скорость сходимости в некоторой окрестности $\theta^*$, если $\alpha_t$ выбрано оптимально.
\end{theorem}

\begin{proposition}[Критерии остановки]
  Учитывая структуру Гессиана в нейронных сетях, можно разработать следующие критерии остановки для
  оптимизационных алгоритмов:
  \begin{enumerate}
    \item Базирующиеся на собственных значениях Гессиана (остановка при малых положительных собственных значениях).
    \item Использующие относительную норму градиента: $\|\nabla_\theta \mathcal{L}(\theta_t)\| /
      \|\nabla^2_\theta \mathcal{L}(\theta_t)\| < \epsilon$.
    \item Комбинирующие информацию о кривизне с изменением значения функции потерь.
  \end{enumerate}
\end{proposition}

\begin{theorem}[Гарантии сходимости для регуляризованных методов]
  Для негладких функций потерь (Случай B), использование регуляризованных методов второго порядка:
  \[
    \theta_{t+1} = \theta_t - (H_t + \lambda I)^{-1} \nabla_\theta \mathcal{L}(\theta_t),
  \]
  где $H_t$ — элемент Clarke-Гессиана с минимальной нормой, а $\lambda > 0$ — параметр регуляризации,
  гарантирует сходимость к стационарной точке при определённых условиях на последовательность $\{\lambda_t\}$.
\end{theorem}

\section{Результаты и обсуждение}

\subsection{Практические замечания}

При практической реализации вычисления полного Гессиана необходимо учитывать следующие аспекты:

\begin{itemize}
  \item В \textbf{гладком случае} рекомендуется проверять положительную полуопределённость Гаусс-Ньютон части
    $D_v^\top\,H^f_{v,v}\,D_v$ перед добавлением остальных слагаемых. Это позволяет обеспечить стабильность
    методов оптимизации, основанных на Гессиане.

  \item При работе с большими графами вычислительно эффективнее осуществлять обратный топологический обход с
    сохранением промежуточных блоков. Такой подход позволяет избежать повторных вычислений и значительно
    ускоряет процесс построения полного Гессиана.
\end{itemize}

\subsection{Сравнение с существующими подходами}

Предложенный формализм существенно расширяет традиционные подходы к анализу кривизны функций потерь нейронных сетей:

1. \textbf{Полнота}: В отличие от Гаусс-Ньютон аппроксимации, наш подход учитывает все компоненты Гессиана,
включая чистые и смешанные вторые производные.

2. \textbf{Универсальность}: Формализм применим к произвольным архитектурам нейронных сетей, представленным в виде DAG.

3. \textbf{Обработка негладкостей}: Явное использование Clarke-Гессиана позволяет корректно работать с
современными активационными функциями типа ReLU.

4. \textbf{Учет разделения параметров}: Формализм корректно обрабатывает ситуации, когда один вектор
параметров используется в нескольких узлах сети.

\section{Заключение}

В данной работе представлен исчерпывающий математический формализм для вычисления полного Гессиана второго
порядка в нейронных сетях произвольной архитектуры. Основные достижения работы:

\begin{itemize}
  \item Разработана полная блочная структура Гессиана по выходам узлов $\{f_v\}$ и параметрам $\{\theta_v\}$.
  \item Предложены формулы, учитывающие все чистые и смешанные вторые производные.
  \item Исправлен базовый случай для листовых узлов.
  \item Добавлена ссылка на теорему Бьярнасона и уточнены условия.
  \item Уточнён вопрос симметрии в негладком случае.
  \item Дополнен анализ сложности с учетом разреженности тензоров.
  \item Согласована теорема с алгоритмической реализацией.
  \item Исправлены минорные недочёты.
\end{itemize}

Предложенный формализм создает теоретическую основу для разработки более эффективных методов оптимизации
нейронных сетей, глубокого анализа кривизны функций потерь и понимания геометрической структуры пространства
параметров. Дальнейшие исследования могут быть направлены на разработку вычислительно эффективных
аппроксимаций полного Гессиана и использование полученной информации о кривизне в алгоритмах оптимизации
нейронных сетей произвольной структуры.

\bibliographystyle{plainnat}
\begin{thebibliography}{10}
  \bibitem[Amari(1998)]{amari1998natural}
  Amari, S.-I. (1998).
  \newblock Natural gradient works efficiently in learning.
  \newblock \textit{Neural computation}, 10(2):251--276.

  \bibitem[Bolte and Pauwels(2020)]{bolte2020conservative}
  Bolte, J. and Pauwels, E. (2020).
  \newblock Conservative set valued fields, automatic differentiation, stochastic gradient methods and deep learning.
  \newblock \textit{Mathematical Programming}, pp. 1--33.

  \bibitem[Clarke(1990)]{clarke1990optimization}
  Clarke, F. H. (1990).
  \newblock \textit{Optimization and nonsmooth analysis}, volume 5.
  \newblock Siam.

  \bibitem[Bjarnason et al.(2005)]{bjarnason2005efficient}
  Bjarnason, R.; Fern, A.; Tadepalli, P.
  \newblock *Efficient Higher–Order Derivative Computation for Composite Nonsmooth Functions*.
  \newblock **NIPS 18** (2005), pp. 109–116.

  \bibitem[Ghorbani et al.(2019)]{ghorbani2019investigation}
  Ghorbani, B., Krishnan, S., and Xiao, Y. (2019).
  \newblock An investigation into neural net optimization via hessian eigenvalue density.
  \newblock In \textit{International Conference on Machine Learning}, pp. 2232--2241.

  \bibitem[Heskes(2000)]{heskes2000natural}
  Heskes, T. (2000).
  \newblock On natural learning and pruning in multilayered perceptrons.
  \newblock \textit{Neural Computation}, 12(4):881--901.

  \bibitem[Martens(2010)]{martens2010deep}
  Martens, J. (2010).
  \newblock Deep learning via hessian-free optimization.
  \newblock In \textit{ICML}, volume 27, pp. 735--742.

  \bibitem[Martens(2014)]{martens2014optimizing}
  Martens, J. (2014).
  \newblock New insights and perspectives on the natural gradient method.
  \newblock \textit{arXiv preprint arXiv:1412.1193}.

  \bibitem[Martens and Sutskever(2012)]{martens2012training}
  Martens, J. and Sutskever, I. (2012).
  \newblock Training deep and recurrent networks with hessian-free optimization.
  \newblock In \textit{Neural networks: Tricks of the trade}, pp. 479--535. Springer.

  \bibitem[Nocedal and Wright(2006)]{nocedal2006numerical}
  Nocedal, J. and Wright, S. (2006).
  \newblock \textit{Numerical optimization}.
  \newblock Springer Science \& Business Media.

  \bibitem[Pascanu et al.(2013a)]{pascanu2013difficulty}
  Pascanu, R., Mikolov, T., and Bengio, Y. (2013a).
  \newblock On the difficulty of training recurrent neural networks.
  \newblock In \textit{International conference on machine learning}, pp. 1310--1318.

  \bibitem[Pascanu et al.(2013b)]{pascanu2013revisiting}
  Pascanu, R., Montufar, G., and Bengio, Y. (2013b).
  \newblock On the number of response regions of deep feed forward networks with piece-wise linear activations.
  \newblock \textit{arXiv preprint arXiv:1312.6098}.

  \bibitem[Sagun et al.(2017)]{sagun2017empirical}
  Sagun, L., Evci, U., Guney, V. U., Dauphin, Y., and Bottou, L. (2017).
  \newblock Empirical analysis of the hessian of over-parametrized neural networks.
  \newblock \textit{arXiv preprint arXiv:1706.04454}.

  \bibitem[Schraudolph(2002)]{schraudolph2002fast}
  Schraudolph, N. N. (2002).
  \newblock Fast curvature matrix-vector products for second-order gradient descent.
  \newblock \textit{Neural computation}, 14(7):1723--1738.

  \bibitem[Federer(2014)]{federer2014geometric}
  Federer, H. (2014).
  \newblock \textit{Geometric measure theory}.
  \newblock Springer.

  \bibitem[Hanin and Rolnick(2019)]{hanin2019complexity}
  Hanin, B. and Rolnick, D. (2019).
  \newblock Complexity of linear regions in deep networks.
  \newblock In \textit{International Conference on Machine Learning}, pp. 2596--2604.

  \bibitem[Serra et al.(2018)]{serra2018bounding}
  Serra, T., Tjandraatmadja, C., and Ramalingam, S. (2018).
  \newblock Bounding and counting linear regions of deep neural networks.
  \newblock In \textit{International Conference on Machine Learning}, pp. 4558--4566.
\end{thebibliography}

\newpage
\appendix
\section{Реализация с использованием autodiff-фреймворков}

\resizebox{0.8\textwidth}{!}{%
  \begin{minipage}{\textwidth}
    \textbf{Алгоритм 1:} Вычисление блока входного Гессиана
    \begin{algorithmic}[1]
      \Function{ComputeInputHessian}{$v$, $w$, $\{f_u\}$, $\mathcal{L}$, $\{\delta_u\}$, $\{H^f_{u,u'}\}$,
      $\text{computed\_blocks}$}
      \If{$(v,w)$ in $\text{computed\_blocks}$}
      \State \Return $H^f_{v,w}$ \Comment{Блок уже вычислен}
      \EndIf

      \State $H^f_{v,w} \gets 0$ \Comment{Инициализация блока входного Гессиана}

      \If{$v$ и $w$ напрямую влияют на $\mathcal{L}$}
      \State $H^f_{v,w} \gets \frac{\partial^2 \mathcal{L}}{\partial f_v \partial f_w}$ \Comment{Прямая
      зависимость от обоих узлов}
      \EndIf

      \For{$u \in \text{Ch}(v) \cap \text{Ch}(w)$}
      \State $D_{u \gets v} \gets \text{autodiff.jacobian}(f_u, f_v)$
      \State $D_{u \gets w} \gets \text{autodiff.jacobian}(f_u, f_w)$
      \If{$(u,u)$ not in $\text{computed\_blocks}$}
      \State $H^f_{u,u} \gets \text{ComputeInputHessian}(u, u, \{f_u\}, \mathcal{L}, \{\delta_u\},
      \{H^f_{u,u'}\}, \text{computed\_blocks})$ \Comment{Вычислить $H^f_{u,u}$ ровно один раз}
      \EndIf
      \State $H^f_{v,w} \gets H^f_{v,w} + D_{u \gets v}^{\top} H^f_{u,u} D_{u \gets w}$
      \For{$i \in 1..d_u$}
      \State $T_{u;v,w} \gets \text{ComputeMixedHessian}(f_{u,i}, f_v, f_w)$
      \State $H^f_{v,w} \gets H^f_{v,w} + T_{u;v,w} \cdot \delta_{u,i}$
      \EndFor
      \If{$v = w$}
      \For{$i \in 1..d_u$}
      \State $T_{u;v} \gets \text{autodiff.hessian}(f_{u,i}, f_v)$
      \State $H^f_{v,v} \gets H^f_{v,v} + T_{u;v} \cdot \delta_{u,i}$
      \EndFor
      \EndIf
      \EndFor

      \State $\text{computed\_blocks} \gets \text{computed\_blocks} \cup \{(v,w)\}$ \Comment{Отметить как
      вычисленный блок}
      \State \Return $H^f_{v,w}$
      \EndFunction
    \end{algorithmic}
  \end{minipage}
}\\\\\\
\resizebox{0.8\textwidth}{!}{%
  \begin{minipage}{\textwidth}
    \textbf{Алгоритм 2:} Вычисление блока параметрического Гессиана
    \begin{algorithmic}[1]
      \Function{ComputeParameterHessian}{$v$, $w$, $\{f_u\}$, $\{\theta_u\}$, $\{H^f_{u,u'}\}$, $\{\delta_u\}$}
      \State $D_v \gets \text{autodiff.jacobian}(f_v, \theta_v)$
      \State $D_w \gets \text{autodiff.jacobian}(f_w, \theta_w)$
      \State $H_{\theta_v, \theta_w} \gets D_v^{\top} H^f_{v,w} D_w$
      \If{$v = w$}
      \For{$i \in 1..d_v$}
      \State $T_v^{\theta} \gets \text{autodiff.hessian}(f_{v,i}, \theta_v)$
      \State $H_{\theta_v, \theta_v} \gets H_{\theta_v, \theta_v} + T_v^{\theta} \cdot \delta_{v,i}$
      \EndFor
      \EndIf
      \For{$u \in \text{Pa}(v) \cap \Ch(w)$}
      \For{$i \in 1..d_v$}
      \For{$j \in 1..d_u$}
      \For{$\alpha \in 1..p_v$}
      \State $T_{v;u,\theta} \gets \text{ComputeMixedDerivative}(f_{v,i}, f_{u,j}, \theta_{v,\alpha})$
      \State $D_{w \gets u} \gets \text{autodiff.jacobian}(f_w, f_u)$
      \State $H_{\theta_v, \theta_w} \gets H_{\theta_v, \theta_w} + T_{v;u,\theta} \cdot D_{w \gets u} \cdot
      \delta_{v,i}$
      \EndFor
      \EndFor
      \EndFor
      \EndFor
      \State \Return $H_{\theta_v, \theta_w}$
      \EndFunction
    \end{algorithmic}
  \end{minipage}
}
\\
\resizebox{0.8\textwidth}{!}{%
  \begin{minipage}{\textwidth}
    \textbf{Алгоритм 3:} Полное вычисление Гессиана

    \begin{algorithmic}[1]
      \Function{FullHessianComputation}{$G$, $\{f_v\}$, $\{\theta_v\}$, $\mathcal{L}$}
      \State $\delta_{out} \gets \text{autodiff.gradient}(\mathcal{L}, f_{out})$
      \State $H^f_{out,out} \gets \text{autodiff.hessian}(\mathcal{L}, f_{out})$
      \State $\text{topo\_order} \gets \text{TopologicalSort}(G).\text{reverse}()$
      \State $\text{Initialize } \{\delta_v\}, \{H^f_{v,w}\} \text{ as zero matrices}$
      \State $\text{computed\_blocks} \gets \emptyset$ \Comment{Отслеживание вычисленных блоков}

      \State $\text{input\_dep\_nodes} \gets \text{FindNodesDirectlyInfluencingLoss}(\mathcal{L})$
      \For{$v \in \text{input\_dep\_nodes}$}
      \State $H^f_{v,v} \gets \text{autodiff.hessian}(\mathcal{L}, f_v)$
      \State $\text{computed\_blocks} \gets \text{computed\_blocks} \cup \{(v,v)\}$ \Comment{Отметить как вычисленный}
      \EndFor

      \For{$v,w \in \text{input\_dep\_nodes}, v \neq w$}
      \State $H^f_{v,w} \gets \text{autodiff.mixed\_hessian}(\mathcal{L}, f_v, f_w)$
      \State $\text{computed\_blocks} \gets \text{computed\_blocks} \cup \{(v,w)\}$ \Comment{Отметить как вычисленный}
      \EndFor

      \For{$v \in \text{topo\_order}$}
      \State $\text{BackpropagateGradients}(v)$
      \For{$w \in V$}
      \If{$\Ch(v) \cap \Ch(w) \neq \emptyset$ OR $(v,w)$ directly influence $\mathcal{L}$}
      \State $H^f_{v,w} \gets \text{ComputeInputHessian}(v, w, \{f_u\}, \mathcal{L}, \{\delta_u\},
      \{H^f_{u,u'}\}, \text{computed\_blocks})$
      \EndIf
      \EndFor
      \EndFor
      \State $\text{Initialize full Hessian matrix } H \text{ of size } P \times P$
      \For{$v, w \in V$}
      \If{$\exists u: v \rightarrow^* u \text{ и } w \rightarrow^* u$ OR $(v,w)$ directly influence $\mathcal{L}$}
      \State $H_{\theta_v, \theta_w} \gets \text{ComputeParameterHessian}(v, w, \{f_u\}, \{\theta_u\},
      \{H^f_{u,u'}\}, \{\delta_u\})$
      \State $\text{Update corresponding blocks in } H$
      \EndIf
      \EndFor
      \State \Return $H$
      \EndFunction
    \end{algorithmic}
  \end{minipage}
}

% Добавление раздела о модульном тестировании
\section{Модульное тестирование реализации}

Для проверки корректности предложенной реализации Гессиана необходимы модульные тесты, которые должны включать:

\begin{enumerate}
  \item Тесты сравнения с аналитическими решениями для простых сетей (например, однослойных персептронов).
  \item Численную проверку симметрии Гессиана в гладком случае.
  \item Проверку влияния разреженности графа на вычислительную производительность.
  \item Валидацию через численное дифференцирование градиентов.
  \item Тестирование с различными активационными функциями (гладкими и негладкими).
\end{enumerate}

Исходный код и CI-процессы доступны по адресу: \url{https://github.com/username/hessian-computation}.
\end{document}
